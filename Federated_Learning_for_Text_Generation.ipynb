{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning for Text Generation\n",
    "\n",
    "This [tutorial](https://www.tensorflow.org/federated/tutorials/federated_learning_for_text_generation) builds on the concepts in the [Federated Learning for Image Classification](https://www.tensorflow.org/federated/tutorials/federated_learning_for_image_classification) tutorial, and demonstrates several other useful approaches for federated learning.\n",
    "\n",
    "In particular, we load a previously trained Keras model, and refine it using federated training on a (simulated) decentralized dataset. This is practically important for several reasons:\n",
    "\n",
    "- The ability to use serialized models makes it easy to mix federated learning with other ML approaches\n",
    "- Further, this allows use of an increasing range of pre-trained models -- for example, training language models from scratch is rarely necessary, as numerous pre-trained models are now widely available (see, e.g., [TF Hub](https://www.tensorflow.org/hub)). Instead, it makes more sense to start from a pre-trained model, and refine it using Federated Learning, adapting to the particular characteristics of the decentralized data for a particular application.\n",
    "\n",
    "For this tutorial, we start with a RNN that generates ASCII characters, and refine it via federated learning. We also show how the final weights can be fed back to the original Keras model, allowing easy evaluation and text generation using standard tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import functools\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/arjun/anaconda3/envs/federated_learning/lib/python3.8/site-packages/tensorflow_federated/python/core/impl/compiler/tensorflow_computation_transformations.py:58: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/arjun/anaconda3/envs/federated_learning/lib/python3.8/site-packages/tensorflow_federated/python/core/impl/compiler/tensorflow_computation_transformations.py:58: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'Hello, World!'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the TFF is working:\n",
    "tff.federated_computation(lambda: 'Hello, World!')()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.4.1\n"
     ]
    }
   ],
   "source": [
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a pre-trained model:\n",
    "\n",
    "We load a model that was pre-trained following the TensorFlow tutorial [Text generation using a RNN with eager execution](https://www.tensorflow.org/tutorials/sequences/text_generation). However, rather than training on [The Complete Works of Shakespeare](http://www.gutenberg.org/files/100/100-0.txt), we pre-trained the model on the text from the Charles Dickens' [A Tale of Two Cities](http://www.ibiblio.org/pub/docs/books/gutenberg/9/98/98.txt) and [A Christmas Carol](http://www.ibiblio.org/pub/docs/books/gutenberg/4/46/46.txt).\n",
    "\n",
    "Other than expanding the vocabulary, we didn't modify the original tutorial, so this initial model isn't state-of-the-art, but it produces reasonable predictions and is sufficient for our tutorial purposes. The final model was saved with [tf.keras.models.save_model(include_optimizer = False)](https://www.tensorflow.org/api_docs/python/tf/keras/models/save_model).\n",
    "\n",
    "We will use federated learning to fine-tune this model for Shakespeare in this tutorial, using a federated version of the data provided by TFF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the vocab lookup tables:\n",
    "\n",
    "# A fixed vocabularly of ASCII chars that occur in the works of Shakespeare and Dickens-\n",
    "vocab = list('dhlptx@DHLPTX $(,048cgkoswCGKOSW[_#\\'/37;?bfjnrvzBFJNRVZ\"&*.26:\\naeimquyAEIMQUY]!%)-159\\r')\n",
    "\n",
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model and generate some text-\n",
    "\n",
    "def load_model(batch_size):\n",
    "    urls = {\n",
    "        1: 'https://storage.googleapis.com/tff-models-public/dickens_rnn.batch1.kerasmodel',\n",
    "        8: 'https://storage.googleapis.com/tff-models-public/dickens_rnn.batch8.kerasmodel'}\n",
    "    assert batch_size in urls, 'batch_size must be in ' + str(urls.keys())\n",
    "    \n",
    "    url = urls[batch_size]\n",
    "    local_file = tf.keras.utils.get_file(os.path.basename(url), origin = url)  \n",
    "    \n",
    "    return tf.keras.models.load_model(local_file, compile = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    # Refer - https://www.tensorflow.org/tutorials/sequences/text_generation\n",
    "    num_generate = 200\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    text_generated = []\n",
    "    temperature = 1.0\n",
    "\n",
    "    model.reset_states()\n",
    "    \n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(\n",
    "            predictions, num_samples=1)[-1, 0].numpy()\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tff-models-public/dickens_rnn.batch1.kerasmodel\n",
      "16195584/16193984 [==============================] - 5s 0us/step\n",
      "What of TensorFlow Federated, you ask? Some of the wine-shop of Monsieur\n",
      "Defarge. \"I saw this. Stelly, my friend, I will recome\n",
      "anywhing before the two fellow, to be born on his. The night was evidently in his\n",
      "misery who were brundable \n"
     ]
    }
   ],
   "source": [
    "# Text generation requires a batch_size=1 model-\n",
    "keras_model_batch1 = load_model(batch_size = 1)\n",
    "\n",
    "print(generate_text(keras_model_batch1, 'What of TensorFlow Federated, you ask? '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess the Federated Shakespeare Data:\n",
    "\n",
    "The [tff.simulation.datasets](https://www.tensorflow.org/federated/api_docs/python/tff/simulation/datasets) package provides a variety of datasets that are split into \"clients\", where each client corresponds to a dataset on a particular device that might participate in federated learning.\n",
    "\n",
    "These datasets provide realistic non-IID data distributions that replicate in simulation the challenges of training on real decentralized data. Some of the pre-processing of this data was done using tools from the [Leaf project](https://arxiv.org/abs/1812.01097) ([github](https://github.com/TalwalkarLab/leaf))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tff-datasets-public/shakespeare.tar.bz2\n",
      "1851392/1848122 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = tff.simulation.datasets.shakespeare.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of available client IDs = 715\n"
     ]
    }
   ],
   "source": [
    "print(f\"number of available client IDs = {len(train_data.client_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets provided by ```shakespeare.load_data()``` consist of a sequence of string ```Tensors```, one for each line spoken by a particular character in a Shakespeare play. The client keys consist of the name of the play joined with the name of the character, so for example ```MUCH_ADO_ABOUT_NOTHING_OTHELLO``` corresponds to the lines for the character Othello in the play Much Ado About Nothing.\n",
    "\n",
    "Note that __in a real federated learning scenario clients are never identified or tracked by ids__, but for simulation it is useful to work with keyed datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"Live regist'red upon our brazen tombs,\\nAnd then grace us in the disgrace of death;\\nWhen, spite of cormorant devouring Time,\\nTh' endeavour of this present breath may buy\\nThat honour which shall bate his scythe's keen edge,\\nAnd make us heirs of all eternity.\\nTherefore, brave conquerors- for so you are\\nThat war against your own affections\\nAnd the huge army of the world's desires-\\nOur late edict shall strongly stand in force:\\nNavarre shall be the wonder of the world;\\nOur court shall be a little Academe,\\nStill and contemplative in living art.\\nYou three, Berowne, Dumain, and Longaville,\\nHave sworn for three years' term to live with me\\nMy fellow-scholars, and to keep those statutes\\nThat are recorded in this schedule here.\\nYour oaths are pass'd; and now subscribe your names,\\nThat his own hand may strike his honour down\\nThat violates the smallest branch herein.\\nIf you are arm'd to do as sworn to do,\\nSubscribe to your deep oaths, and keep it too.\\nYour oath is pass'd to pass away from these.\"\n",
      "\n",
      "b'this?\\nDid you hear the proclamation?'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example data from King Lear.\n",
    "# Here the play is \"The Tragedy of King Lear\" and the character is \"King\".\n",
    "raw_example_dataset = train_data.create_tf_dataset_for_client('THE_TRAGEDY_OF_KING_LEAR_KING')\n",
    "\n",
    "# To allow for future extensions, each entry 'x' is an 'OrderedDict' with a single key 'snippets' which contains the text-\n",
    "for x in raw_example_dataset.take(2):\n",
    "    print(f\"{x['snippets']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use [tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) transformations to prepare this data for training the char RNN loaded above-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input pre-processing parameters\n",
    "SEQ_LENGTH = 100\n",
    "BATCH_SIZE = 8\n",
    "BUFFER_SIZE = 100  # For dataset shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a lookup table to map string chars to indexes, using the vocab loaded above-\n",
    "table = tf.lookup.StaticHashTable(\n",
    "    tf.lookup.KeyValueTensorInitializer(\n",
    "        keys = vocab, values = tf.constant(list(range(len(vocab))), dtype = tf.int64)),\n",
    "    default_value=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_ids(x):\n",
    "    s = tf.reshape(x['snippets'], shape=[1])\n",
    "    chars = tf.strings.bytes_split(s).values\n",
    "    ids = table.lookup(chars)\n",
    "    return ids\n",
    "\n",
    "\n",
    "def split_input_target(chunk):\n",
    "    input_text = tf.map_fn(lambda x: x[:-1], chunk)\n",
    "    target_text = tf.map_fn(lambda x: x[1:], chunk)\n",
    "    return (input_text, target_text)\n",
    "\n",
    "\n",
    "def preprocess(dataset):\n",
    "    return (\n",
    "        # Map ASCII chars to int64 indexes using the vocab\n",
    "        dataset.map(to_ids)\n",
    "        # Split into individual chars\n",
    "        .unbatch()\n",
    "        # Form example sequences of SEQ_LENGTH +1\n",
    "        .batch(SEQ_LENGTH + 1, drop_remainder=True)\n",
    "        # Shuffle and form minibatches\n",
    "        .shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "        # And finally split into (input, target) tuples,\n",
    "        # each of length SEQ_LENGTH.\n",
    "        .map(split_input_target)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the formation of the original sequences and in the formation of batches above, we use ```drop_remainder = True``` for simplicity. This means that any characters (clients) that don't have at least ```(SEQ_LENGTH + 1) * BATCH_SIZE``` chars of text will have empty datasets. A typical approach to address this would be to pad the batches with a special token, and then mask the loss to not take the padding tokens into account.\n",
    "\n",
    "This would complicate the example somewhat, so for this tutorial we only use full batches, as in the [standard tutorial](https://www.tensorflow.org/tutorials/sequences/text_generation). However, in the federated setting this issue is more significant, because many users might have small datasets.\n",
    "\n",
    "Now we can preprocess our ```raw_example_dataset```, and check the types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(8, 100), dtype=tf.int64, name=None), TensorSpec(shape=(8, 100), dtype=tf.int64, name=None))\n"
     ]
    }
   ],
   "source": [
    "example_dataset = preprocess(raw_example_dataset)\n",
    "print(example_dataset.element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model and test on the preprocessed data:\n",
    "\n",
    "We loaded an uncompiled keras model, but in order to run ```keras_model.evaluate```, we need to compile it with a loss and metrics. We will also compile in an optimizer, which will be used as the on-device optimizer in Federated Learning.\n",
    "\n",
    "The original tutorial didn't have char-level accuracy (the fraction of predictions where the highest probability was put on the correct next char). This is a useful metric, so we add it. However, we need to define a new metric class for this because our predictions have rank 3 (a vector of logits for each of the ```BATCH_SIZE * SEQ_LENGTH``` predictions), and ```SparseCategoricalAccuracy``` expects only rank 2 predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenedCategoricalAccuracy(tf.keras.metrics.SparseCategoricalAccuracy):\n",
    "\n",
    "    def __init__(self, name = 'accuracy', dtype = tf.float32):\n",
    "        super().__init__(name, dtype = dtype)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight = None):\n",
    "        y_true = tf.reshape(y_true, [-1, 1])\n",
    "        y_pred = tf.reshape(y_pred, [-1, len(vocab), 1])\n",
    "        return super().update_state(y_true, y_pred, sample_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on an exmaple of Shakespeare character = 39.750%\n"
     ]
    }
   ],
   "source": [
    "# Now we can compile a model, and evaluate it on our 'example_dataset'-\n",
    "BATCH_SIZE = 8  # The training and eval batch size for the rest of this tutorial.\n",
    "keras_model = load_model(batch_size = BATCH_SIZE)\n",
    "\n",
    "keras_model.compile(\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n",
    "    metrics = [FlattenedCategoricalAccuracy()])\n",
    "\n",
    "# Confirm that loss is much lower on Shakespeare as compared to random data-\n",
    "loss, accuracy = keras_model.evaluate(example_dataset.take(5), verbose = 0)\n",
    "print(f\"Evaluating on an exmaple of Shakespeare character = {accuracy * 100:.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected accuracy for random guessing = 1.163%\n"
     ]
    }
   ],
   "source": [
    "# As a sanity check, we can construct some completely random data, where we expect the accuracy to be essentially random-\n",
    "random_guessed_accuracy = 1.0 / len(vocab)\n",
    "\n",
    "print(f\"Expected accuracy for random guessing = {random_guessed_accuracy * 100:.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on completely random data = 1.125%\n"
     ]
    }
   ],
   "source": [
    "random_indexes = np.random.randint(low = 0, high = len(vocab), size = 1 * BATCH_SIZE * (SEQ_LENGTH + 1))\n",
    "\n",
    "data = collections.OrderedDict(\n",
    "    snippets = tf.constant(\n",
    "        ''.join(np.array(vocab)[random_indexes]), shape = [1, 1])\n",
    ")\n",
    "\n",
    "random_dataset = preprocess(tf.data.Dataset.from_tensor_slices(data))\n",
    "loss, accuracy = keras_model.evaluate(random_dataset, steps = 10, verbose = 0)\n",
    "print(f\"Evaluating on completely random data = {accuracy * 100:.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune the model with Federated Learning:\n",
    "\n",
    "TFF serializes all TensorFlow computations so they can potentially be run in a non-Python environment (even though __at the moment, only a simulation runtime implemented in Python is available__).\n",
    "\n",
    "Even though we are running in eager mode, (TF 2.0), currently TFF serializes TensorFlow computations by constructing the necessary ops inside the context of a \"```with tf.Graph.as_default()```\" statement. Thus, we need to provide a function that TFF can use to introduce our model into a graph it controls. We do this as follows-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the keras_model inside 'create_tff_model()', which TFF will call to produce a new copy of the model inside the graph that it will \n",
    "# serialize. Note: we want to construct all the necessary objects we'll need 'inside' this method-\n",
    "def create_tff_model():\n",
    "    # TFF uses an 'input_spec' so it knows the types and shapes that your model expects\n",
    "    input_spec = example_dataset.element_spec\n",
    "    keras_model_clone = tf.keras.models.clone_model(keras_model)\n",
    "    \n",
    "    return tff.learning.from_keras_model(\n",
    "        keras_model_clone, input_spec = input_spec,\n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n",
    "        metrics = [FlattenedCategoricalAccuracy()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to construct a Federated Averaging iterative process, which we will use to improve the model (for details on the Federated Averaging algorithm, see the paper [Communication-Efficient Learning of Deep Networks from Decentralized Data](https://arxiv.org/abs/1602.05629)).\n",
    "\n",
    "We use a compiled Keras model to perform standard (non-federated) evaluation after each round of federated training. This is useful for research purposes when doing simulated federated learning and there is a standard test dataset.\n",
    "\n",
    "In a realistic production setting this same technique might be used to take models trained with federated learning and evaluate them on a centralized benchmark dataset for testing or quality assurance purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['gru_1/gru_cell/kernel:0', 'gru_1/gru_cell/recurrent_kernel:0', 'gru_1/gru_cell/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['gru_1/gru_cell/kernel:0', 'gru_1/gru_cell/recurrent_kernel:0', 'gru_1/gru_cell/bias:0'] when minimizing the loss.\n"
     ]
    }
   ],
   "source": [
    "# This command builds all the TensorFlow graphs and serializes them-\n",
    "fed_avg = tff.learning.build_federated_averaging_process(\n",
    "    model_fn = create_tff_model,\n",
    "    client_optimizer_fn = lambda: tf.keras.optimizers.SGD(lr = 0.5)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the simplest possible loop, where we run federated averaging for one round on a single client on a single batch-\n",
    "state = fed_avg.initialize()\n",
    "state, metrics = fed_avg.next(state, [example_dataset.take(5)])\n",
    "\n",
    "train_metrics = metrics['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 4.4012 & accuracy = 13.950%\n"
     ]
    }
   ],
   "source": [
    "print(f\"loss = {train_metrics['loss']:.4f} & accuracy = {train_metrics['accuracy'] * 100:.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's write a slightly more interesting training and evaluation loop.\n",
    "\n",
    "So that this simulation still runs relatively quickly, we train on the same three clients each round, only considering two minibatches for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data(client, source = train_data):\n",
    "    return preprocess(source.create_tf_dataset_for_client(client)).take(5)\n",
    "\n",
    "\n",
    "clients = [\n",
    "    'ALL_S_WELL_THAT_ENDS_WELL_CELIA', 'MUCH_ADO_ABOUT_NOTHING_OTHELLO',\n",
    "]\n",
    "\n",
    "train_datasets = [data(client) for client in clients]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We concatenate the test datasets for evaluation with Keras by creating a Dataset of Datasets, and then identity flat mapping across all of the\n",
    "# examples-\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    [data(client, test_data) for client in clients]).flat_map(lambda x: x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial state of the model produced by ```fed_avg.initialize()``` is based on the random initializers for the Keras model, not the weights that were loaded, since ```clone_model()``` does not clone the weights.\n",
    "\n",
    "To start training from a pre-trained model, we set the model weights in the server state directly from the loaded model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ROUNDS = 5\n",
    "\n",
    "# The state of the FL server, containing the model and optimization state-\n",
    "state = fed_avg.initialize()\n",
    "\n",
    "# Load our pre-trained Keras model weights into the global model state.\n",
    "state = tff.learning.state_with_new_model_weights(\n",
    "    state,\n",
    "    trainable_weights=[v.numpy() for v in keras_model.trainable_weights],\n",
    "    non_trainable_weights = [\n",
    "        v.numpy() for v in keras_model.non_trainable_weights\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def keras_evaluate(state, round_num):\n",
    "    # Take our global model weights and push them back into a Keras model to use its standard '.evaluate()' method.\n",
    "    keras_model = load_model(batch_size = BATCH_SIZE)\n",
    "    keras_model.compile(\n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n",
    "        metrics = [FlattenedCategoricalAccuracy()])\n",
    "    \n",
    "    state.model.assign_weights_to(keras_model)\n",
    "    loss, accuracy = keras_model.evaluate(example_dataset, steps = 2, verbose = 0)\n",
    "    print(f\"Evaluation: loss = {loss:.4f} & accuracy = {accuracy * 100:.3f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round = 0\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f3b386d9430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f3b386d9430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: loss = 5.3592 & accuracy = 1.938%\n",
      "Training: loss = 6.9824 & accuracy = 12.188%\n",
      "Final evaluation\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f3b385d3940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f3b385d3940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: loss = 5.2523 & accuracy = 3.375%\n",
      "round = 1\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f3b385dc3a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f3b385dc3a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: loss = 5.2217 & accuracy = 3.500%\n",
      "Training: loss = 6.7846 & accuracy = 12.425%\n",
      "Final evaluation\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f3b681c41f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f3b681c41f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: loss = 5.8037 & accuracy = 0.563%\n",
      "round = 2\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f3b38571ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f3b38571ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: loss = 5.7766 & accuracy = 1.187%\n",
      "Training: loss = 6.4419 & accuracy = 10.100%\n",
      "Final evaluation\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f3b6b5ec310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f3b6b5ec310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: loss = 5.3337 & accuracy = 3.250%\n",
      "round = 3\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f3b385f5ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f3b385f5ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: loss = 5.2514 & accuracy = 3.562%\n",
      "Training: loss = 5.9402 & accuracy = 12.538%\n",
      "Final evaluation\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f3b681e5ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f3b681e5ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: loss = 5.7109 & accuracy = 0.750%\n",
      "round = 4\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f3b38748160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f3b38748160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: loss = 5.7754 & accuracy = 0.875%\n",
      "Training: loss = 6.4232 & accuracy = 11.575%\n",
      "Final evaluation\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f3b387d70d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f3b387d70d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: loss = 5.4294 & accuracy = 1.125%\n"
     ]
    }
   ],
   "source": [
    "for round_num in range(NUM_ROUNDS):\n",
    "    print(f\"round = {round_num}\")\n",
    "    keras_evaluate(state, round_num)\n",
    "    state, metrics = fed_avg.next(state, train_datasets)\n",
    "    train_metrics = metrics['train']\n",
    "    print(f\"Training: loss = {train_metrics['loss']:.4f} & accuracy = {train_metrics['accuracy'] * 100:.3f}%\")\n",
    "    \n",
    "    print('Final evaluation')\n",
    "    keras_evaluate(state, NUM_ROUNDS + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the default changes, we haven't done enough training to make a big difference, but if you train longer on more Shakespeare data, you should see a difference in the style of the text generated with the updated model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What of TensorFlow Federated, you ask? Says\n",
      "a night, Mr. Lorry?\" said Mr. Lorry. \"Say need my a single woman adorad, precisult the Project Gutenberg Literary Archive Foundation\n",
      "     A ThA FULomemo at this moment-deceive that valutious\n",
      "p\n"
     ]
    }
   ],
   "source": [
    "# Set our newly trained weights back in the originally created model-\n",
    "keras_model_batch1.set_weights([v.numpy() for v in keras_model.weights])\n",
    "\n",
    "# Text generation requires batch_size = 1\n",
    "print(generate_text(keras_model_batch1, 'What of TensorFlow Federated, you ask? '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suggested extensions:\n",
    "\n",
    "This tutorial is just the first step! Here are some ideas for how you might try extending this notebook:\n",
    "\n",
    "- Write a more realistic training loop where you sample clients to train on randomly.\n",
    "\n",
    "- Use \"```.repeat(NUM_EPOCHS)```\" on the client datasets to try multiple epochs of local training (e.g., as in [McMahan et. al.](https://arxiv.org/abs/1602.05629)). See also [Federated Learning for Image Classification]https://www.tensorflow.org/federated/tutorials/federated_learning_for_image_classification) which does this.\n",
    "\n",
    "- Change the ```compile()``` command to experiment with using different optimization algorithms on the client.\n",
    "\n",
    "- Try the ```server_optimizer``` argument to ```build_federated_averaging_process``` to try different algorithms for applying the model updates on the server.\n",
    "\n",
    "- Try the ```client_weight_fn``` argument to to ```build_federated_averaging_process``` to try different weightings of the clients. The default weights client updates by the number of examples on the client, but you can do e.g. ```client_weight_fn = lambda _: tf.constant(1.0)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
