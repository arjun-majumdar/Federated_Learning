{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d674f9cd",
   "metadata": {},
   "source": [
    "# Federated Learning:\n",
    "\n",
    "An example code using MNIST dataset.\n",
    "\n",
    "__Federated Learning is mostly suited for parameterized learning — this includes all variants of neural networks__. In contrast, traditional Machine Learning models like Decision Trees, KNN, etc. which just store the training data while training might not fully utilize the potentials of Federated Learning.\n",
    "\n",
    "\n",
    "__Refer:__\n",
    "\n",
    "https://towardsdatascience.com/federated-learning-a-step-by-step-implementation-in-tensorflow-aac568283399"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d46ba7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "from imutils import paths\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import AveragePooling2D, Conv2D, MaxPooling2D, ReLU\n",
    "from tensorflow.keras import models, layers, datasets\n",
    "from tensorflow.keras.layers import Dense, Flatten, Reshape, Input, InputLayer\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f0fdcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tutorial_master.fl_mnist_implementation_tutorial_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b000689f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbed66b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "083d551d",
   "metadata": {},
   "source": [
    "### Reading and preprocessing MNIST data set:\n",
    "\n",
    "The JPEG version of MNIST data set can be downloaded from [Kaggle](https://www.kaggle.com/scolianni/mnistasjpg). It consists of 42000 digit images with each class kept in\n",
    "separate folder. The data will be loaded into memory with the code below. For validation data, 10% of this data is used for the finally trained global model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a36c539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(paths, verbose = -1):\n",
    "    '''\n",
    "    Funtion to load MNIST JPEG digit images stored in it's respective folders.\n",
    "    All gigist belonging to class '9' will exist in a directory named '9'\n",
    "    \n",
    "    Returns a list of loaded images as numpy arrays.\n",
    "    '''\n",
    "    data = list()\n",
    "    labels = list()\n",
    "    \n",
    "    # loop over the input images\n",
    "    for (i, imgpath) in enumerate(paths):\n",
    "        # Read in image as greyscale-\n",
    "        im_gray = cv2.imread(imgpath, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        # Flatten image due to MLP neural network-\n",
    "        image = np.array(im_gray).flatten()\n",
    "        \n",
    "        # Extract label using path to directory-\n",
    "        label = imgpath.split(os.path.sep)[-2]\n",
    "        \n",
    "        # Normalize image to be in the range [0, 1]-\n",
    "        data.append(image/255)\n",
    "        \n",
    "        # Add label to list-\n",
    "        labels.append(label)\n",
    "        \n",
    "        # show an update every 'verbose' images\n",
    "        if verbose > 0 and i > 0 and (i + 1) % verbose == 0:\n",
    "            print(\"[INFO] processed {}/{}\".format(i + 1, len(paths)))\n",
    "            \n",
    "    # return a tuple of the data and labels\n",
    "    return data, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a630aa08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7a4fce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] processed 10000/42000\n",
      "[INFO] processed 20000/42000\n",
      "[INFO] processed 30000/42000\n",
      "[INFO] processed 40000/42000\n"
     ]
    }
   ],
   "source": [
    "# Specify the absolute path for MNIST training dataset folder-\n",
    "\n",
    "# Folder containing training images-\n",
    "# img_path = 'D:\\\\Other_Files\\\\Research_Works\\\\MNIST_dataset\\\\trainingSample\\\\trainingSample\\\\'\n",
    "# NOTE: There are 60 images each per folder. Hence, total number of training images = 60 x 10 = 600\n",
    "\n",
    "img_path = \"D:\\\\Other_Files\\\\Research_Works\\\\MNIST_dataset\\\\trainingSet\\\\trainingSet\\\\\"\n",
    "\n",
    "# Obtain directory using path object-\n",
    "image_paths = list(paths.list_images(img_path))\n",
    "\n",
    "# Load images and labels-\n",
    "image_list, label_list = load(image_paths, verbose = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c412dcf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 42000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check-\n",
    "len(image_list), len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5eab81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a3b3c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels using 1-hot-encode the labels-\n",
    "lb = LabelBinarizer()\n",
    "label_list = lb.fit_transform(label_list)\n",
    "\n",
    "# Split dataset into training and testing sets-\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    image_list, label_list, \n",
    "    test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f7eccf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, numpy.ndarray, list, numpy.ndarray)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train), type(y_train), type(X_test), type(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87d44cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37800, 10), (4200, 10))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fc19706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37800, 4200)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a4584af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from np array to list-\n",
    "y_train = list(y_train)\n",
    "y_test = list(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58ce180e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37800"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14b510a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907a0079",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172d6d26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51eca3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "print(f\"X_train.shape = {X_train.shape} & y_train.shape = {y_train.shape}\")\n",
    "print(f\"X_test.shape = {X_test.shape} & y_test.shape = {y_test.shape}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9207189b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4705bba2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "588b6278",
   "metadata": {},
   "source": [
    "### Federated Members (clients) as Data Shards:\n",
    "\n",
    "For real world Federated Learningapplication/implementation, each federated member usually has it's own data associated with it in isolation. The aim of FL is to deploy models to data and not vice-versa. The shard creation step shown here happens only for experiments.\n",
    "\n",
    "For this particular code example, the training set is shared/divided into 10 shards, one for each FL client. 'create_clients()' aims to achieve this sharding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00a52502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clients(image_list, label_list, num_clients = 10, initial = 'client'):\n",
    "    '''\n",
    "    return: a dictionary with key = clients' names and value = data shards\n",
    "    \n",
    "    input args: \n",
    "    \n",
    "    image_list: a list of numpy arrays of training images\n",
    "    label_list:a list of binarized labels for each image\n",
    "    num_client: number of fedrated members (clients)\n",
    "    initials: the clients'name prefix, e.g, client_1\n",
    "    '''\n",
    "\n",
    "    # Create a list of client names-\n",
    "    client_names = ['{}_{}'.format(initial, i + 1) for i in range(num_clients)]\n",
    "\n",
    "    # Randomly shuffle image data-\n",
    "    data = list(zip(image_list, label_list))\n",
    "    random.shuffle(data)\n",
    "\n",
    "    # Split/Shard image data and place each at each client-\n",
    "    size = len(data) // num_clients\n",
    "    shards = [data[i: i + size] for i in range(0, size*num_clients, size)]\n",
    "\n",
    "    # Make sure that: number of clients must equal number of shards-\n",
    "    assert(len(shards) == len(client_names))\n",
    "\n",
    "    return {client_names[i] : shards[i] for i in range(len(client_names))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff74aeaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11ef6c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create clients-\n",
    "clients = create_clients(X_train, y_train, num_clients = 10, initial = 'client')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be7a005c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client name: client_1 has 3780 training data\n",
      "client name: client_2 has 3780 training data\n",
      "client name: client_3 has 3780 training data\n",
      "client name: client_4 has 3780 training data\n",
      "client name: client_5 has 3780 training data\n",
      "client name: client_6 has 3780 training data\n",
      "client name: client_7 has 3780 training data\n",
      "client name: client_8 has 3780 training data\n",
      "client name: client_9 has 3780 training data\n",
      "client name: client_10 has 3780 training data\n"
     ]
    }
   ],
   "source": [
    "for client_name in clients.keys():\n",
    "    print(f\"client name: {client_name} has {len(clients[client_name])} training data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0d77a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7fcf44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63261c70",
   "metadata": {},
   "source": [
    "### Processing and batching clients’ and test data:\n",
    "\n",
    "Process all of the client's shards/data into TF dataset and then batch them-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "591c5233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(data_shard, batch_size = 32):\n",
    "    '''\n",
    "    Creates tf.Dataset object using a client's 'data_shard'\n",
    "    \n",
    "    Args:\n",
    "    shard: a data, label tuple constituting a client's data shard\n",
    "    batch_size :batch size\n",
    "    \n",
    "    Returns:\n",
    "    tfds object\n",
    "    '''\n",
    "    \n",
    "    # Split a data shard as tuple separate data and labels lists-\n",
    "    data, label = zip(*data_shard)\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n",
    "    \n",
    "    return dataset.shuffle(len(label)).batch(batch_size = batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f3dc9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "715b9a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and batch training for each client-\n",
    "clients_batched = {}\n",
    "\n",
    "for client_name in clients.keys():\n",
    "    clients_batched[client_name] = batch_data(clients[client_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "204e6891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and batch testing set as well-\n",
    "test_batched = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size = len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fb9b27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99f2abe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82cc1af0",
   "metadata": {},
   "source": [
    "### Design LeNet-300-100 Dense Neural Network:\n",
    "\n",
    "Remember that, FL is mostly suited for parameterized learning — all types of neural networks. Machine learning techniques such as KNN or it likes that merely store training data while learning might not benefit from FL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b065d79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lenet300():\n",
    "    \"\"\"\n",
    "    Function to define the architecture of a neural network model\n",
    "    following 300 100 architecture for MNIST dataset.\n",
    "    \n",
    "    Output: Returns designed and compiled neural network model\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(InputLayer(input_shape = (784, )))\n",
    "    \n",
    "    # model.add(Flatten())\n",
    "    \n",
    "    model.add(\n",
    "        Dense(units = 300, activation = 'relu', kernel_initializer=tf.initializers.GlorotUniform())\n",
    "    )\n",
    "    \n",
    "    # model.add(l.Dropout(0.2))\n",
    "    \n",
    "    model.add(\n",
    "        Dense(units = 100, activation = 'relu', kernel_initializer=tf.initializers.GlorotUniform()),\n",
    "    )\n",
    "    \n",
    "    # model.add(l.Dropout(0.1))\n",
    "    \n",
    "    model.add(\n",
    "        Dense(units = 10, activation = 'softmax'),\n",
    "    )\n",
    "    \n",
    "    '''\n",
    "    'comms_round' is the number of global epochs or, aggregations which will be executed/performed during training.\n",
    "    \n",
    "    Rather than decaying the learning rate with respect to the number of local epochs as you might be familiar with,\n",
    "    here, the learning rate is decayed with respect to the number of global aggregation.\n",
    "    \n",
    "    This is a hyper parameter which can be found with experiments.\n",
    "    \n",
    "    Refer to the research paper for more details and theory-\n",
    "    \"Federated Learning with Non-IID Data\" by Yue Zhao et al.\n",
    "    '''\n",
    "    comms_round = 100\n",
    "    \n",
    "    lr = 0.01\n",
    "    \n",
    "    # Compile model-\n",
    "    model.compile(\n",
    "        loss = tf.keras.losses.categorical_crossentropy,\n",
    "        \n",
    "        # optimizer='adam',\n",
    "        # optimizer = tf.keras.optimizers.Adam(lr = 0.0012),\n",
    "        optimizer = tf.keras.optimizers.SGD(\n",
    "            lr = lr, decay = lr / comms_round,\n",
    "            momentum = 0.9\n",
    "        ),\n",
    "        \n",
    "        metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2c5397",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aa63ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c32fba9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c47f3a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31518d7f",
   "metadata": {},
   "source": [
    "### Model Aggregation (Federated Averaging):\n",
    "\n",
    "The pre-processing steps until now can be considered to be part of a typical deep learning piple with the exception of data sharding/partitioning and the client creation part.\n",
    "\n",
    "Now, the focus is shifted to the _vanilla version of Federated Learning_. The data being used is horizontally partitioned, so we will simply be doing component wise parameter averaging which will be weighed based on the proportion of data points contributed by each participating client.\n",
    "\n",
    "The federated averaging equation being used is based on the research paper \"Communication-Efficient Learning of Deep Networks from Decentralized Data\" by H. Brendan McMahan and is as follows:\n",
    "\n",
    "$f\\left(x\\right) = \\sum_{k=1}^{K}\\frac{n_k}{n}$ where $F_k\\left(w\\right) = \\frac{1}{n_k}\\sum_{i \\in P_k} f_i\\left(w\\right)$\n",
    "\n",
    "On the right hand side, we are estimating the weight parameters for each client, based on the loss values recorded across every data point it was trained with. And, on the left, we scaled each of those parameters and sum all of them, component-wise.\n",
    "\n",
    "To visualize the difference between horizontally vs. vertically partitioned data:\n",
    "<img src=\"D:\\\\Other_Files\\\\Research_Works\\\\Horizontally-and-vertically-partitioned-data.png\">\n",
    "[image source](https://www.researchgate.net/figure/Horizontally-and-vertically-partitioned-data-Horizontal-partitions-have-different_fig1_283708993)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632c6728",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da981f48",
   "metadata": {},
   "source": [
    "'weight_scaling_factor()' function calculates the proportion of a client’s local training data with the overall training data held by all of the clients.\n",
    "\n",
    "+ This approach is _not_ applicable in for a real world application. For a real world application, the training data will be disjointed and consequently, no single client can correctly estimate the quantity of the combined set.\n",
    "\n",
    "+ In such a case, each client will be expected to indicate the number of data points it trained with while updating the server with new parameters after each local training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "02465aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This procedure is implemented as 3 functions:\n",
    "\n",
    "def weight_scalling_factor(clients_trn_data, client_name):\n",
    "    client_names = list(clients_trn_data.keys())\n",
    "    \n",
    "    # Get batch_size-\n",
    "    batch_size = list(clients_trn_data[client_name])[0][0].shape[0]\n",
    "    \n",
    "    # Calculate the total number of training data points across all of the clients-\n",
    "    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names]) * batch_size\n",
    "    \n",
    "    # Get the total number of data points held by a client-\n",
    "    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() * batch_size\n",
    "    \n",
    "    # Calculate the scaling factor as a fraction-\n",
    "    return local_count / global_count\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18686410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1959d384",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "85ec74bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'scale_model_weights()' function scales each of the local model’s weights based on the value of their scaling factor\n",
    "# which was calculated in the equation shown above.\n",
    "def scale_model_weights(weight, scalar):\n",
    "    '''\n",
    "    Python function for scaling a model's weights\n",
    "    '''\n",
    "    weight_final = []\n",
    "    steps = len(weight)\n",
    "    \n",
    "    for i in range(steps):\n",
    "        weight_final.append(scalar * weight[i])\n",
    "    \n",
    "    return weight_final\n",
    "\n",
    "\n",
    "# 'sum_scaled_weights()' sums all of the clients’ scaled weights together.\n",
    "def sum_scaled_weights(scaled_weight_list):\n",
    "    '''\n",
    "    Return the sum of the listed scaled weights. The is equivalent to scaled average of the weights\n",
    "    '''\n",
    "    avg_grad = list()\n",
    "    \n",
    "    # Get the average grad accross all client gradients\n",
    "    for grad_list_tuple in zip(*scaled_weight_list):\n",
    "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
    "        avg_grad.append(layer_mean)\n",
    "        \n",
    "    return avg_grad\n",
    "\n",
    "\n",
    "def test_model(X_test, Y_test,  model, comm_round):\n",
    "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits = True)\n",
    "    \n",
    "    #logits = model.predict(X_test, batch_size=100)\n",
    "    logits = model.predict(X_test)\n",
    "    \n",
    "    loss = cce(Y_test, logits)\n",
    "    \n",
    "    acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))\n",
    "    \n",
    "    # Remember that: 'comms_round' is the number of global epochs or, aggregations which will be executed/performed during training.\n",
    "    # print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))\n",
    "    print(f\"comm_round = {comm_round}, global val_accuracy = {acc:.2f}% and global val_loss = {loss:.4f}\")\n",
    "     \n",
    "    return acc, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ab75cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce14caa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b390b665",
   "metadata": {},
   "source": [
    "## Federated Model Training:\n",
    "\n",
    "The training logic has two main loops, the outer loop is for the global iteration, while the inner loop is for iterating through client’s local training. There’s also an implicit third loop which accounts for the local epochs and will be taken care of by the 'epochs' argument in the 'model.fit()' method.\n",
    "\n",
    "There are 10 clients, each running 1 local epoch on top of 100 global communication rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cc135fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'comms_round' is the number of global epochs or, aggregations which will be executed/performed during training-\n",
    "comms_round = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dc61e90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comm_round = 0, global val_accuracy = 0.88% and global val_loss = 1.6677\n",
      "comm_round = 1, global val_accuracy = 0.90% and global val_loss = 1.6172\n",
      "comm_round = 2, global val_accuracy = 0.91% and global val_loss = 1.5951\n",
      "comm_round = 3, global val_accuracy = 0.93% and global val_loss = 1.5781\n",
      "comm_round = 4, global val_accuracy = 0.94% and global val_loss = 1.5657\n",
      "comm_round = 5, global val_accuracy = 0.94% and global val_loss = 1.5560\n",
      "comm_round = 6, global val_accuracy = 0.95% and global val_loss = 1.5483\n",
      "comm_round = 7, global val_accuracy = 0.95% and global val_loss = 1.5434\n",
      "comm_round = 8, global val_accuracy = 0.95% and global val_loss = 1.5399\n",
      "comm_round = 9, global val_accuracy = 0.96% and global val_loss = 1.5336\n",
      "comm_round = 10, global val_accuracy = 0.96% and global val_loss = 1.5304\n",
      "comm_round = 11, global val_accuracy = 0.96% and global val_loss = 1.5273\n",
      "comm_round = 12, global val_accuracy = 0.96% and global val_loss = 1.5244\n",
      "comm_round = 13, global val_accuracy = 0.96% and global val_loss = 1.5215\n",
      "comm_round = 14, global val_accuracy = 0.96% and global val_loss = 1.5197\n",
      "comm_round = 15, global val_accuracy = 0.96% and global val_loss = 1.5179\n",
      "comm_round = 16, global val_accuracy = 0.96% and global val_loss = 1.5168\n",
      "comm_round = 17, global val_accuracy = 0.96% and global val_loss = 1.5150\n",
      "comm_round = 18, global val_accuracy = 0.97% and global val_loss = 1.5129\n",
      "comm_round = 19, global val_accuracy = 0.96% and global val_loss = 1.5120\n"
     ]
    }
   ],
   "source": [
    "# Initialize the global model-\n",
    "global_model = create_lenet300()\n",
    "\n",
    "\n",
    "# Begin global training loop-\n",
    "for comm_round in range(comms_round):\n",
    "    \n",
    "    # Get the global model's weights - this will act as the initial weights for all local models\n",
    "    global_weights = global_model.get_weights()\n",
    "    \n",
    "    # Initial list to collect local model weights after scaling-\n",
    "    scaled_local_weight_list = list()\n",
    "\n",
    "    # Randomize client data - using keys\n",
    "    # Shuffle the clients dictionary order to ensure randomness\n",
    "    client_names = list(clients_batched.keys())\n",
    "    random.shuffle(client_names)\n",
    "    \n",
    "    # Iterate through client training.\n",
    "    # Loop through each client and create a new local corresponding model-\n",
    "    for client in client_names:\n",
    "        # smlp_local = SimpleMLP()\n",
    "        # local_model = smlp_local.build(784, 10)\n",
    "        local_model = create_lenet300()\n",
    "        '''\n",
    "        local_model.compile(loss=loss, \n",
    "                      optimizer=optimizer, \n",
    "                      metrics=metrics)\n",
    "        '''\n",
    "        \n",
    "        # Set local model weights to the weights of the global model-\n",
    "        local_model.set_weights(global_weights)\n",
    "        \n",
    "        # Train local model with client's local data\n",
    "        # local model (client) is then trained for one epoch\n",
    "        local_model.fit(clients_batched[client], epochs = 1, verbose = 0)\n",
    "        \n",
    "        # After training, the new weights are scaled and appended to the 'scaled_local_weight_list'\n",
    "        # Scale the model weights and add to list-\n",
    "        scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
    "        scaled_local_weight_list.append(scaled_weights)\n",
    "        \n",
    "        # Clear session to free memory after each communication round-\n",
    "        K.clear_session()\n",
    "    \n",
    "    \n",
    "    # Sum up all of the scaled local trained weights (by components) and update the global model to this new aggregate.\n",
    "    # To compute the average for all of the local model, we simply take the sum of the scaled weights-\n",
    "    average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "    \n",
    "    # Update global model \n",
    "    global_model.set_weights(average_weights)\n",
    "\n",
    "    # Test global model and print out metrics after each communication round-\n",
    "    for(X_test, Y_test) in test_batched:\n",
    "        global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a44e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38db116",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1ea08f7",
   "metadata": {},
   "source": [
    "## SGD Vs Federated Averaging:\n",
    "\n",
    "The FL model results are appreciable at 96.5% validation accuracy after 20 communication rounds. But how does this compare to a standard SGD model trained on the same data set?\n",
    "\n",
    "To find out, we will train a single 3-layer MLP model (rather 10 as we did in FL) on the combined training data. Remember the combined data was our training data prior to partitioning.\n",
    "\n",
    "To ensure an equal playing ground, every hyper-parameter is  retained which were used for the FL training except the batch size. Rather than using 32 , our SGD’s batch size will be 320. With this setting, we are sure that the SGD model would see the exact  same number of training samples per epoch as the global model did per communication round in FL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9455f499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comm_round = 1, global val_accuracy = 0.96% and global val_loss = 1.5139\n"
     ]
    }
   ],
   "source": [
    "SGD_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(y_train)).batch(320)\n",
    "\n",
    "model_SGD = create_lenet300()\n",
    "\n",
    "# Train the model on training data using SGD=\n",
    "history = model_SGD.fit(SGD_dataset, epochs = 20, verbose = 0)\n",
    "\n",
    "# Test the SGD global model on validation data-\n",
    "for(X_test, Y_test) in test_batched:\n",
    "        SGD_acc, SGD_loss = test_model(X_test, Y_test, model_SGD, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caaf478",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8f992a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f806644",
   "metadata": {},
   "source": [
    "### Independent and identically distributed random variables:\n",
    "\n",
    "In probability theory and statistics, a collection of random variables is independent and identically distributed if:\n",
    "\n",
    "- each random variable has the same probability distribution as the others (and)\n",
    "\n",
    "- all are mutually independent\n",
    "\n",
    "This property is usually abbreviated as i.i.d. or iid or IID. This property is usually abbreviated as i.i.d. or iid or IID. Herein, i.i.d. is used, because it is the most prevalent.\n",
    "\n",
    "[source](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c7e9d2",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "If FL neural network outperforms SGD based training, keep in mind that these kind of results are not likely in real world scenario. __Real world federated data held by clients are mostly NON independent and identically distributed (IID)__.\n",
    "\n",
    "For example, we could have replicated this scenario by constructing our client shards defined above such that each shard comprises of images from a single class — e.g client_1 having only images of digit 1, client_2 having only images of digit 2 and so on. This arrangement would have lead to a significant reduction in the performance of the FL model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7d30e3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_iid_x(image_list, label_list, x = 1, num_intraclass_clients = 10):\n",
    "        '''\n",
    "        Python function to shard any classification data in a non-IID manner.\n",
    "        This creates 'x' non_IID clients\n",
    "        \n",
    "        args: \n",
    "            image_list: python list of images or data points\n",
    "            label_list: python list of labels\n",
    "            x: none IID severity, 1 means each client will only have one class of data\n",
    "            num_intraclass_client: number of sub-client to be created from each none IID class,\n",
    "            e.g for x=1, we could create 10 further clients by splitting each class into 10\n",
    "            \n",
    "        return - dictionary \n",
    "            keys - clients's name, \n",
    "            value - client's non iid 1 data shard (as tuple list of images and labels)\n",
    "        '''\n",
    "        \n",
    "        non_iid_x_clients = dict()\n",
    "        \n",
    "        # create unique label list and shuffle\n",
    "        unique_labels = np.unique(np.array(label_list))\n",
    "        random.shuffle(unique_labels)\n",
    "        \n",
    "        # create sub label lists based on x\n",
    "        sub_lab_list = [unique_labels[i:i + x] for i in range(0, len(unique_labels), x)]\n",
    "            \n",
    "        for item in sub_lab_list:\n",
    "            class_data = [(image, label) for (image, label) in zip(image_list, label_list) if label in item]\n",
    "            \n",
    "            # decouple tuple list into seperate image and label lists\n",
    "            images, labels = zip(*class_data)\n",
    "            \n",
    "            # create formated client initials\n",
    "            initial = ''\n",
    "            for lab in item:\n",
    "                initial = initial + lab + '_'\n",
    "            \n",
    "            # create num_intraclass_clients clients from the class \n",
    "            intraclass_clients = create_clients(list(images), list(labels), num_intraclass_clients, initial)\n",
    "            \n",
    "            # append intraclass clients to main clients'dict\n",
    "            non_iid_x_clients.update(intraclass_clients)\n",
    "        \n",
    "        return non_iid_x_clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa5cf8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468615ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "224b65cd",
   "metadata": {},
   "source": [
    "### Research papers:\n",
    "\n",
    "\n",
    "+ Federated Learning with Non-IID Data, Yue Zhao et al, arXiv: 1806.00582v1\n",
    "\n",
    "+ Communication-Efficient Learning of Deep Networks from Decentralized Data, H. Brendan McMahan et al, arXiv:1602.05629v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab42bb88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
